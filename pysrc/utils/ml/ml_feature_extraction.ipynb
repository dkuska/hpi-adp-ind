{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, auc, roc_auc_score, precision_score, balanced_accuracy_score, fbeta_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(json_content):\n",
    "\n",
    "    baseline_stats = []\n",
    "    baseline_inds = []\n",
    "\n",
    "    for run in json_content['runs']:\n",
    "        \n",
    "        config = run['configuration']\n",
    "        source_files = config['source_files']\n",
    "        samplingrates = config['sampling_rates']\n",
    "        \n",
    "        samplingmethods = config['sampling_methods']\n",
    "        is_baseline = config['is_baseline']\n",
    "        \n",
    "        column_statistics = run['column_statistics']\n",
    "        for col_stats in column_statistics:\n",
    "        \n",
    "            table_name = col_stats['column_information']['table_name']\n",
    "            column_name = col_stats['column_information']['column_name']\n",
    "            count = col_stats['count']\n",
    "            unique_count = col_stats['unique_count']\n",
    "            unique_ratio = col_stats['unique_ratio']\n",
    "            \n",
    "            if is_baseline:\n",
    "                baseline_stats.append([table_name, column_name, count, unique_count, unique_ratio, 1, 'None'])\n",
    "        \n",
    "        results = run['results']\n",
    "        for result in results['inds']:\n",
    "            dependent = result['dependents'][0]\n",
    "            dependent_table_name = dependent['table_name']\n",
    "            dependent_column_name = dependent['column_name']\n",
    "            referenced = result['referenced'][0]\n",
    "            referenced_table_name = referenced['table_name']\n",
    "            referenced_column_name = referenced['column_name']\n",
    "            \n",
    "            if is_baseline:\n",
    "                baseline_inds.append([dependent_table_name, dependent_column_name, referenced_table_name, referenced_column_name])\n",
    "    baseline_df = pd.DataFrame(baseline_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "\n",
    "    return baseline_df, baseline_inds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_stats(json_content, baseline_df, baseline_inds):\n",
    "    sampled_stats = []\n",
    "    ind_dfs = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for run in json_content['runs']:\n",
    "        \n",
    "        config = run['configuration']\n",
    "        source_files = config['source_files']\n",
    "        samplingrates = config['sampling_rates']\n",
    "        \n",
    "        samplingmethods = config['sampling_methods']\n",
    "        is_baseline = config['is_baseline']\n",
    "        if is_baseline:\n",
    "            continue\n",
    "        \n",
    "        run_stats = []\n",
    "        column_statistics = run['column_statistics']\n",
    "        for col_stats in column_statistics:\n",
    "        \n",
    "            table_name = col_stats['column_information']['table_name']\n",
    "            column_name = col_stats['column_information']['column_name']\n",
    "            count = col_stats['count']\n",
    "            unique_count = col_stats['unique_count']\n",
    "            unique_ratio = col_stats['unique_ratio']\n",
    "            \n",
    "            if is_baseline:\n",
    "                run_stats.append([table_name, column_name, count, unique_count, unique_ratio, 1, 'None'])\n",
    "            else:\n",
    "                run_stats.append([table_name, column_name, count, unique_count, unique_ratio, samplingrates[0], samplingmethods[0]])\n",
    "        \n",
    "        sampled_stats.extend(run_stats)\n",
    "        stats_df = pd.DataFrame(run_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "        \n",
    "        run_inds = []\n",
    "        \n",
    "        results = run['results']\n",
    "        for result in results['inds']:\n",
    "            dependent = result['dependents'][0]\n",
    "            dependent_table_name = dependent['table_name']\n",
    "            dependent_column_name = dependent['column_name']\n",
    "            referenced = result['referenced'][0]\n",
    "            referenced_table_name = referenced['table_name']\n",
    "            referenced_column_name = referenced['column_name']\n",
    "            \n",
    "            if is_baseline:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                if [dependent_table_name, dependent_column_name, referenced_table_name, referenced_column_name] in baseline_inds:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "                    \n",
    "            missing_values = result['errors'][0]['missing_values']\n",
    "        \n",
    "            run_inds.append([dependent_table_name, dependent_column_name, referenced_table_name, referenced_column_name, missing_values])\n",
    "        \n",
    "        \n",
    "        # baseline_df = pd.DataFrame(baseline_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "        run_df = pd.DataFrame(run_inds, columns=['left_table', 'left_column', 'right_table', 'right_column', 'missing_values'])\n",
    "        \n",
    "        # Merge with Baseline Stats\n",
    "        # Left\n",
    "        merged_df = pd.merge(run_df, baseline_df, left_on=['left_table', 'left_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count', 'sampling_rate', 'sampling_method'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'left_baseline_count', \n",
    "                        'unique_ratio': 'left_baseline_unique_ratio',\n",
    "                        'sampling_rate': 'left_baseline_sampling_rate'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        # Right\n",
    "        merged_df = pd.merge(merged_df, baseline_df, left_on=['right_table', 'right_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count', 'sampling_rate', 'sampling_method'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'right_baseline_count', \n",
    "                        'unique_ratio': 'right_baseline_unique_ratio',\n",
    "                        'sampling_rate': 'right_baseline_sampling_rate'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        \n",
    "        # Merge with sampled stats\n",
    "        # Left\n",
    "        merged_df = pd.merge(merged_df, stats_df, left_on=['left_table', 'left_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'left_count', \n",
    "                        'unique_ratio': 'left_unique_ratio',\n",
    "                        'sampling_rate': 'left_sampling_rate',\n",
    "                        'sampling_method': 'left_sampling_method'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        # Right\n",
    "        merged_df = pd.merge(merged_df, stats_df, left_on=['right_table', 'right_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'right_count', \n",
    "                        'unique_ratio': 'right_unique_ratio',\n",
    "                        'sampling_rate': 'right_sampling_rate',\n",
    "                        'sampling_method': 'right_sampling_method'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        ind_dfs.append(merged_df)\n",
    "        \n",
    "    # sampled_stats_df = pd.DataFrame(sampled_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "\n",
    "    features = pd.concat(ind_dfs)\n",
    "    features.drop(['right_table', 'right_column', 'left_table', 'left_column'], axis='columns', inplace=True)\n",
    "    \n",
    "    labels = pd.DataFrame(labels, columns=['labels'])\n",
    "    \n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(features):\n",
    "    ohe_cols = features['left_sampling_method'].to_numpy().reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(sparse_output=False).fit(ohe_cols)\n",
    "    ohe_df = ohe.transform(ohe_cols)\n",
    "\n",
    "    ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out())\n",
    "\n",
    "    features.drop(['left_sampling_method'], axis='columns', inplace=True)\n",
    "    features = pd.concat([features.reset_index().reindex(ohe_df.index), ohe_df], axis=1)\n",
    "    return features\n",
    "\n",
    "\n",
    "def drop_useless_features(features):\n",
    "    # return features.drop(['left_sampling_method', 'right_sampling_method', 'left_unique_ratio', 'right_unique_ratio'], axis='columns')\n",
    "    return features.drop(['right_sampling_method', 'left_unique_ratio', 'right_unique_ratio'], axis='columns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(json_content):\n",
    "    baseline_df, baseline_inds = get_baseline(json_content)\n",
    "    features, labels = get_run_stats(json_content, baseline_df, baseline_inds)\n",
    "    features = drop_useless_features(features)\n",
    "    \n",
    "    features['cardinality_ratio'] = features['left_baseline_count'] / features['right_baseline_count']\n",
    "    features['sample_size_ratio'] = features['left_count'] / features['right_count']\n",
    "    # features['missing_ratio'] = features['missing_values'] / features['left_count']\n",
    "    # features['useless_ratio'] = features['missing_values'] / features['right_count']\n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "def get_features_labels_v2(json_content):\n",
    "    baseline_df, baseline_inds = get_baseline(json_content)\n",
    "    features, labels = get_run_stats(json_content, baseline_df, baseline_inds)\n",
    "    return features, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all JSONs in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_CENSUS.json\n",
      "data_ENSEMBL.json\n",
      "data_COMA.json\n",
      "data_TESMA.json\n",
      "data_CATH.json\n",
      "data_TPCH.json\n",
      "data_SCOP.json\n"
     ]
    }
   ],
   "source": [
    "features, labels = [], []\n",
    "\n",
    "for f in os.listdir('.'):\n",
    "    if os.path.isfile(f):\n",
    "        if 'json' in f:\n",
    "            print(f)\n",
    "            with open(f) as file:\n",
    "                json_content = json.load(file)\n",
    "                \n",
    "                X, y = get_features_labels(json_content)\n",
    "                features.append(X)\n",
    "                labels.append(y)\n",
    "\n",
    "features = pd.concat(features)\n",
    "features = ohe(features)\n",
    "features.drop(['index'], axis='columns', inplace=True)\n",
    "# Scaling features\n",
    "features = pd.DataFrame(MaxAbsScaler().fit_transform(features), columns=features.columns)\n",
    "\n",
    "# column_titles = ['missing_values', \n",
    "#  'left_count', 'left_sampling_rate', 'left_baseline_count', 'left_baseline_unique_ratio',\n",
    "#  'right_count', 'right_sampling_rate', 'right_baseline_count', 'right_baseline_unique_ratio']\n",
    "\n",
    "# features = features.reindex(columns=column_titles)\n",
    "\n",
    "labels = pd.concat(labels)\n",
    "\n",
    "features.to_csv('features.csv')\n",
    "labels.to_csv('labels.csv')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1, stratify=labels)\n",
    "\n",
    "y_train = y_train.to_numpy().ravel()\n",
    "y_test = y_test.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(518102, 17)\n",
      "labels\n",
      "0         507375\n",
      "1          10727\n",
      "dtype: int64\n",
      "Index(['missing_values', 'left_baseline_count', 'left_baseline_unique_ratio',\n",
      "       'right_baseline_count', 'right_baseline_unique_ratio', 'left_count',\n",
      "       'left_sampling_rate', 'right_count', 'right_sampling_rate',\n",
      "       'cardinality_ratio', 'sample_size_ratio', 'x0_biggest-value',\n",
      "       'x0_evenly-spaced', 'x0_first', 'x0_longest-value', 'x0_random',\n",
      "       'x0_smallest-value'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.value_counts())\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('features.csv')\n",
    "labels.to_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.cov()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
