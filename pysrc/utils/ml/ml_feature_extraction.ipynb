{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, auc, roc_auc_score, precision_score, balanced_accuracy_score, fbeta_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline(json_content):\n",
    "\n",
    "    baseline_stats = []\n",
    "    baseline_inds = []\n",
    "\n",
    "    for run in json_content['runs']:\n",
    "        \n",
    "        config = run['configuration']\n",
    "        is_baseline = config['is_baseline']\n",
    "        if is_baseline:\n",
    "            column_statistics = run['column_statistics']\n",
    "            for col_stats in column_statistics:\n",
    "            \n",
    "                table_name = col_stats['column_information']['table_name']\n",
    "                column_name = col_stats['column_information']['column_name']\n",
    "                count = col_stats['count']\n",
    "                unique_count = col_stats['unique_count']\n",
    "                unique_ratio = col_stats['unique_ratio']\n",
    "                \n",
    "                baseline_stats.append([table_name, column_name, count, unique_count, unique_ratio, 1, 'None'])\n",
    "            \n",
    "            results = run['results']\n",
    "            for result in results['inds']:\n",
    "                dependent = result['dependents'][0]\n",
    "                dependent_table_name = dependent['table_name']\n",
    "                dependent_column_name = dependent['column_name']\n",
    "                referenced = result['referenced'][0]\n",
    "                referenced_table_name = referenced['table_name']\n",
    "                referenced_column_name = referenced['column_name']\n",
    "                \n",
    "                baseline_inds.append([dependent_table_name, dependent_column_name, referenced_table_name, referenced_column_name])\n",
    "                \n",
    "    baseline_df = pd.DataFrame(baseline_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "\n",
    "    return baseline_df, baseline_inds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_stats(json_content, baseline_df, baseline_inds):\n",
    "    sampled_stats = []\n",
    "    ind_dfs = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for run in json_content['runs']:\n",
    "        \n",
    "        config = run['configuration']\n",
    "        source_files = config['source_files']\n",
    "        \n",
    "        sampling_method = config['sampling_methods'][0]\n",
    "        total_budget = config['total_budget']\n",
    "        \n",
    "        is_baseline = config['is_baseline']\n",
    "        if is_baseline:\n",
    "            continue\n",
    "        \n",
    "        run_stats = []\n",
    "        column_statistics = run['column_statistics']\n",
    "        for col_stats in column_statistics:\n",
    "        \n",
    "            table_name = col_stats['column_information']['table_name']\n",
    "            column_name = col_stats['column_information']['column_name']\n",
    "            count = col_stats['count']\n",
    "            unique_count = col_stats['unique_count']\n",
    "            unique_ratio = col_stats['unique_ratio']\n",
    "            \n",
    "            sampling_rate =  count/ baseline_df.loc[(baseline_df['table_name'] == table_name) & (baseline_df['column_name'] == column_name)]['count'].iloc[0]\n",
    "            \n",
    "            if is_baseline:\n",
    "                run_stats.append([table_name, column_name, count, unique_count, unique_ratio, 1, 'None'])\n",
    "            else:\n",
    "                run_stats.append([table_name, column_name, count, unique_count, unique_ratio, sampling_rate, sampling_method])\n",
    "        \n",
    "        sampled_stats.extend(run_stats)\n",
    "        stats_df = pd.DataFrame(run_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "        \n",
    "        run_inds = []\n",
    "        \n",
    "        results = run['results']\n",
    "        for result in results['inds']:\n",
    "            dependent = result['dependents'][0]\n",
    "            dependent_table_name = dependent['table_name']\n",
    "            dependent_column_name = dependent['column_name']\n",
    "            referenced = result['referenced'][0]\n",
    "            referenced_table_name = referenced['table_name']\n",
    "            referenced_column_name = referenced['column_name']\n",
    "            \n",
    "            if is_baseline:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                if [dependent_table_name, dependent_column_name, referenced_table_name, referenced_column_name] in baseline_inds:\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "                    \n",
    "            missing_values = result['errors'][0]['missing_values']\n",
    "        \n",
    "            run_inds.append([dependent_table_name, dependent_column_name, referenced_table_name, referenced_column_name, missing_values])\n",
    "        \n",
    "        \n",
    "        # baseline_df = pd.DataFrame(baseline_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "        run_df = pd.DataFrame(run_inds, columns=['left_table', 'left_column', 'right_table', 'right_column', 'missing_values'])\n",
    "        \n",
    "        # Merge with Baseline Stats\n",
    "        # Left\n",
    "        merged_df = pd.merge(run_df, baseline_df, left_on=['left_table', 'left_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count', 'sampling_rate', 'sampling_method'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'left_baseline_count', \n",
    "                        'unique_ratio': 'left_baseline_unique_ratio',\n",
    "                        'sampling_rate': 'left_baseline_sampling_rate'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        # Right\n",
    "        merged_df = pd.merge(merged_df, baseline_df, left_on=['right_table', 'right_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count', 'sampling_rate', 'sampling_method'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'right_baseline_count', \n",
    "                        'unique_ratio': 'right_baseline_unique_ratio',\n",
    "                        'sampling_rate': 'right_baseline_sampling_rate'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        \n",
    "        # Merge with sampled stats\n",
    "        # Left\n",
    "        merged_df = pd.merge(merged_df, stats_df, left_on=['left_table', 'left_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'left_count', \n",
    "                        'unique_ratio': 'left_unique_ratio',\n",
    "                        'sampling_rate': 'left_sampling_rate',\n",
    "                        'sampling_method': 'left_sampling_method'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        # Right\n",
    "        merged_df = pd.merge(merged_df, stats_df, left_on=['right_table', 'right_column'], right_on=['table_name', 'column_name'])\n",
    "        merged_df.drop(['table_name', 'column_name', 'unique_count'], axis='columns', inplace=True)\n",
    "        merged_df.rename({'count': 'right_count', \n",
    "                        'unique_ratio': 'right_unique_ratio',\n",
    "                        'sampling_rate': 'right_sampling_rate',\n",
    "                        'sampling_method': 'right_sampling_method'}, \n",
    "                        axis='columns',\n",
    "                        inplace=True)\n",
    "        ind_dfs.append(merged_df)\n",
    "        \n",
    "    # sampled_stats_df = pd.DataFrame(sampled_stats, columns=['table_name', 'column_name', 'count', 'unique_count', 'unique_ratio', 'sampling_rate', 'sampling_method'])\n",
    "\n",
    "    features = pd.concat(ind_dfs)\n",
    "    features.drop(['right_table', 'right_column', 'left_table', 'left_column'], axis='columns', inplace=True)\n",
    "    \n",
    "    \n",
    "    labels = pd.DataFrame(labels, columns=['labels'])\n",
    "    \n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(features):\n",
    "    ohe_cols = features['left_sampling_method'].to_numpy().reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(sparse_output=False).fit(ohe_cols)\n",
    "    ohe_df = ohe.transform(ohe_cols)\n",
    "\n",
    "    ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out())\n",
    "\n",
    "    features.drop(['left_sampling_method'], axis='columns', inplace=True)\n",
    "    features = pd.concat([features.reset_index().reindex(ohe_df.index), ohe_df], axis=1)\n",
    "    return features\n",
    "\n",
    "\n",
    "def drop_useless_features(features):\n",
    "    return features.drop(['left_sampling_method', 'right_sampling_method', 'left_unique_ratio', 'right_unique_ratio'], axis='columns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(json_content):\n",
    "    baseline_df, baseline_inds = get_baseline(json_content)\n",
    "    features, labels = get_run_stats(json_content, baseline_df, baseline_inds)\n",
    "    features = drop_useless_features(features)\n",
    "    \n",
    "    features['cardinality_ratio'] = features['left_baseline_count'] / features['right_baseline_count']\n",
    "    features['sample_size_ratio'] = features['left_count'] / features['right_count']\n",
    "    # features['missing_ratio'] = features['missing_values'] / features['left_count']\n",
    "    # features['useless_ratio'] = features['missing_values'] / features['right_count']\n",
    "    \n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_with_exclusion_criteria(X):\n",
    "    \n",
    "    indices = []\n",
    "    for index, row in X.iterrows():\n",
    "        if row['left_baseline_count'] > row['right_baseline_count']:\n",
    "            continue\n",
    "        \n",
    "        if row['missing_values'] > (row['right_baseline_count'] * row['right_baseline_unique_ratio']) - row['right_count']:\n",
    "            continue\n",
    "        \n",
    "        indices.append(index)\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over all JSONs in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_TCPH.json\n",
      "data_CENSUS.json\n",
      "data_ENSEMBL.json\n",
      "data_COMA.json\n",
      "data_CATH.json\n",
      "data_SCOP.json\n"
     ]
    }
   ],
   "source": [
    "features, labels = {}, {}\n",
    "\n",
    "for f in os.listdir('.'):\n",
    "    if os.path.isfile(f):\n",
    "        if 'json' in f:\n",
    "            print(f)\n",
    "            with open(f) as file:\n",
    "                json_content = json.load(file)\n",
    "                \n",
    "                X, y = get_features_labels(json_content)\n",
    "                indices = filter_with_exclusion_criteria(X)\n",
    "                X = pd.DataFrame(X.iloc[indices], columns=X.columns)\n",
    "                y = pd.DataFrame(y.iloc[indices], columns=y.columns)\n",
    "                features[f] = X\n",
    "                labels[f] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for key in features.keys():\n",
    "    X.append(features[key])\n",
    "    y.append(labels[key])\n",
    "    \n",
    "X = pd.concat(X)\n",
    "scaler = MaxAbsScaler()\n",
    "scaler = scaler.fit(X)\n",
    "\n",
    "with open('maxabsscaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_values</th>\n",
       "      <th>left_baseline_count</th>\n",
       "      <th>left_baseline_unique_ratio</th>\n",
       "      <th>right_baseline_count</th>\n",
       "      <th>right_baseline_unique_ratio</th>\n",
       "      <th>left_count</th>\n",
       "      <th>left_sampling_rate</th>\n",
       "      <th>right_count</th>\n",
       "      <th>right_sampling_rate</th>\n",
       "      <th>cardinality_ratio</th>\n",
       "      <th>sample_size_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing_values, left_baseline_count, left_baseline_unique_ratio, right_baseline_count, right_baseline_unique_ratio, left_count, left_sampling_rate, right_count, right_sampling_rate, cardinality_ratio, sample_size_ratio]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['data_COMA.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCPH\n",
      "CENSUS\n",
      "ENSEMBL\n",
      "CATH\n",
      "SCOP\n"
     ]
    }
   ],
   "source": [
    "for key in features.keys():\n",
    "    fname = key.rsplit('.')[0].split('_')[-1]\n",
    "    if fname == 'COMA':\n",
    "        continue\n",
    "    print(fname)\n",
    "    # # feats = .reset_index(drop=True))\n",
    "    feats = pd.DataFrame(scaler.transform(features[key]), columns = features[key].columns).reset_index(drop=True)\n",
    "    feats.to_csv(f\"features_{fname}.csv\")\n",
    "    labels[key].reset_index(drop=True).to_csv(f\"labels_{fname}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "0         328441\n",
      "1           4546\n",
      "dtype: int64\n",
      "labels\n",
      "0         161229\n",
      "1           4023\n",
      "dtype: int64\n",
      "labels\n",
      "0         331967\n",
      "1           4767\n",
      "dtype: int64\n",
      "labels\n",
      "0         201252\n",
      "1           1713\n",
      "dtype: int64\n",
      "labels\n",
      "0         330635\n",
      "1           4695\n",
      "dtype: int64\n",
      "labels\n",
      "0         306311\n",
      "1           4091\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "source_files = [f for f in os.listdir('.') if os.path.isfile(f) and 'json' in f]\n",
    "\n",
    "for source_file_combination in combinations(source_files, len(source_files)-1):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    not_included_file = [f for f in source_files if f not in source_file_combination][0].rsplit('.')[0].split('_')[1]\n",
    "    folder = './no_' + not_included_file + '/'\n",
    "    \n",
    "    for source_file in source_file_combination:\n",
    "        X.append(features[source_file])\n",
    "        y.append(labels[source_file])\n",
    "    \n",
    "    X = pd.concat(X)\n",
    "    # Scaling features\n",
    "    X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "    # column_titles = ['missing_values', \n",
    "    #                  'left_baseline_count', 'left_baseline_unique_ratio',\n",
    "    #                  'right_baseline_count', 'right_baseline_unique_ratio', \n",
    "    #                  'left_count', 'left_sampling_rate', \n",
    "    #                  'right_count', 'right_sampling_rate',\n",
    "    #                  'cardinality_ratio', 'sample_size_ratio', \n",
    "    #                  'missing_ratio', 'useless_ratio']\n",
    "    # X = X.reindex(columns=column_titles)\n",
    "    y = pd.concat(y)\n",
    "    print(y.value_counts())\n",
    "\n",
    "    # X.reset_index(drop=True).to_csv(folder + 'features.csv')\n",
    "    # y.reset_index(drop=True).to_csv(folder + 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_TCPH.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sampling_rates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(f) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     json_content \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m---> 10\u001b[0m     X, y \u001b[39m=\u001b[39m get_features_labels(json_content)\n\u001b[1;32m     11\u001b[0m     features\u001b[39m.\u001b[39mappend(X)\n\u001b[1;32m     12\u001b[0m     labels\u001b[39m.\u001b[39mappend(y)\n",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m, in \u001b[0;36mget_features_labels\u001b[0;34m(json_content)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_features_labels\u001b[39m(json_content):\n\u001b[0;32m----> 2\u001b[0m     baseline_df, baseline_inds \u001b[39m=\u001b[39m get_baseline(json_content)\n\u001b[1;32m      3\u001b[0m     features, labels \u001b[39m=\u001b[39m get_run_stats(json_content, baseline_df, baseline_inds)\n\u001b[1;32m      4\u001b[0m     features \u001b[39m=\u001b[39m drop_useless_features(features)\n",
      "Cell \u001b[0;32mIn [2], line 10\u001b[0m, in \u001b[0;36mget_baseline\u001b[0;34m(json_content)\u001b[0m\n\u001b[1;32m      8\u001b[0m config \u001b[39m=\u001b[39m run[\u001b[39m'\u001b[39m\u001b[39mconfiguration\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m source_files \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39msource_files\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m samplingrates \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39msampling_rates\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     12\u001b[0m samplingmethods \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39msampling_methods\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m is_baseline \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mis_baseline\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sampling_rates'"
     ]
    }
   ],
   "source": [
    "# features = pd.concat(features)\n",
    "# features = ohe(features)\n",
    "# features.drop(['index'], axis='columns', inplace=True)\n",
    "# # Scaling features\n",
    "# features = pd.DataFrame(MaxAbsScaler().fit_transform(features), columns=features.columns)\n",
    "\n",
    "# # column_titles = ['missing_values', \n",
    "# #  'left_count', 'left_sampling_rate', 'left_baseline_count', 'left_baseline_unique_ratio',\n",
    "# #  'right_count', 'right_sampling_rate', 'right_baseline_count', 'right_baseline_unique_ratio']\n",
    "\n",
    "# # features = features.reindex(columns=column_titles)\n",
    "# labels = pd.concat(labels)\n",
    "\n",
    "# features.to_csv('features.csv')\n",
    "# labels.to_csv('labels.csv')\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1, stratify=labels)\n",
    "\n",
    "# y_train = y_train.to_numpy().ravel()\n",
    "# y_test = y_test.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('features.csv')\n",
    "labels.to_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.cov()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
