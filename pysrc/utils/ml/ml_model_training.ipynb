{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, auc, roc_auc_score, precision_score, balanced_accuracy_score, fbeta_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    print(f\"Recall:            {recall_score(y, y_pred)}\")\n",
    "    print(f\"Precision:         {precision_score(y, y_pred, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mfeatures.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m labels \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mlabels.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(features, labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, stratify\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m      6\u001b[0m y_train \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mravel()\n\u001b[1;32m      7\u001b[0m y_test \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2583\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2579\u001b[0m         CVClass \u001b[39m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2581\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2583\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X\u001b[39m=\u001b[39;49marrays[\u001b[39m0\u001b[39;49m], y\u001b[39m=\u001b[39;49mstratify))\n\u001b[1;32m   2585\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2586\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2587\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1689\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \n\u001b[1;32m   1661\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[39mto an integer.\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1689\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1690\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2078\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2076\u001b[0m class_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2077\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmin(class_counts) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 2078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2079\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe least populated class in y has only 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2080\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m member, which is too few. The minimum\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2081\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m number of groups for any class cannot\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m be less than 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2083\u001b[0m     )\n\u001b[1;32m   2085\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m<\u001b[39m n_classes:\n\u001b[1;32m   2086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe train_size = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m should be greater or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mequal to the number of classes = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2089\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "\n",
    "# features = pd.read_csv('features.csv')\n",
    "# labels = pd.read_csv('labels.csv')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1, stratify=labels)\n",
    "\n",
    "# y_train = y_train.to_numpy().ravel()\n",
    "# y_test = y_test.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_scorer = make_scorer(fbeta_score, beta=0.0001)\n",
    "    \n",
    "def find_best_hyperparam(estimator, param_distributions, X_train, y_train):\n",
    "    clf = RandomizedSearchCV(estimator=estimator, \n",
    "                             param_distributions=param_distributions, \n",
    "                             scoring=fbeta_scorer,\n",
    "                             n_iter=20,\n",
    "                             cv=5, \n",
    "                             verbose=2, \n",
    "                             n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.best_params_\n",
    "\n",
    "X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         950440\n",
      "1          73526\n",
      "dtype: int64\n",
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         481845\n",
      "1          36946\n",
      "dtype: int64\n",
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         876637\n",
      "1          56196\n",
      "dtype: int64\n",
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         851575\n",
      "1          74370\n",
      "dtype: int64\n",
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         742655\n",
      "1          74370\n",
      "dtype: int64\n",
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         959849\n",
      "1          74370\n",
      "dtype: int64\n",
      "   labels\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "labels\n",
      "0         898685\n",
      "1          56442\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sub_folders = [f for f in os.listdir('.') if os.path.isdir(f) and 'no_' in f]\n",
    "\n",
    "for sub_folder in sub_folders:\n",
    "    features = pd.read_csv('./' + sub_folder + '/features.csv', index_col=0)\n",
    "    labels = pd.read_csv('./' + sub_folder + '/labels.csv', index_col=0)\n",
    "    print(labels.value_counts())\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1, stratify=labels)\n",
    "    y_train = y_train.to_numpy().ravel()\n",
    "    y_test = y_test.to_numpy().ravel()\n",
    "    \n",
    "    \n",
    "    # Scale\n",
    "    X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    # Logistic Regression\n",
    "    from sklearn.linear_model import LogisticRegressionCV\n",
    "    logreg_cv = LogisticRegressionCV(cv=5, random_state=0).fit(X_resampled, y_resampled)\n",
    "    \n",
    "    print(\"Logistic Regression\")\n",
    "    scoring(logreg_cv, X_test, y_test)\n",
    "    \n",
    "    # Retrain\n",
    "    \n",
    "    # with open('./' + sub_folder + 'logreg_model.pkl', 'wb') as f:\n",
    "    #     pickle.dump(logreg_cv, f)\n",
    "    \n",
    "    # Random Forest\n",
    "    params = {\n",
    "    'max_depth': [10, 20, 30, 40, 60, 80, 100],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "    best_forest, best_tree_params = find_best_hyperparam(RandomForestClassifier(), params, X_resampled, y_resampled)\n",
    "    \n",
    "    print(best_tree_params.get_params())\n",
    "    print(\"Forest\")\n",
    "    scoring(best_forest, X_test, y_test)\n",
    "    with open('./' + sub_folder + 'forest_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_forest, f)\n",
    "        \n",
    "\n",
    "    \n",
    "    # SVC\n",
    "    params = {'C': [1, 10, 100], \n",
    "          'gamma': [0.001, 0.0001], \n",
    "          'kernel': ['rbf', 'linear'],}\n",
    "\n",
    "    best_svc, best_svc_params = find_best_hyperparam(SVC(max_iter=5000, probability=True, class_weight = 'balanced'), params, X_resampled, y_resampled)\n",
    "    print(best_svc.get_params())\n",
    "    \n",
    "    print(\"SVC\")\n",
    "    scoring(best_svc, X_test, y_test)\n",
    "    with open('./' + sub_folder + 'svc_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_svc, f)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Note MDI is computed on training set\n",
    "# ## MDI has bias towards features with high cardinalities\n",
    "\n",
    "# def plot_MDI(forest):\n",
    "#     importances = forest.feature_importances_\n",
    "#     std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "#     forest_importances = pd.Series(importances, index=features.columns)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "#     ax.set_title(\"Feature importances using MDI\")\n",
    "#     ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "# def plot_permutation_feature_importance(forest, X_test, y_test):\n",
    "#     result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    \n",
    "#     forest_importances = pd.Series(result.importances_mean, index=features.columns)\n",
    "    \n",
    "#     fig, ax = plt.subplots()\n",
    "#     forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "#     ax.set_title(\"Feature importances using permutation on full model\")\n",
    "#     ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_models(X_train, y_train, X_test, y_test):\n",
    "#     print(\"Decision Tree\")\n",
    "#     clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "\n",
    "#     print(\"Random Forest\")\n",
    "#     clf = RandomForestClassifier(max_depth=4, random_state=0).fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "#     # plot_MDI(clf)\n",
    "#     # plot_permutation_feature_importance(clf, X_test, y_test)\n",
    "\n",
    "#     print(\"Logistic Regression\")\n",
    "#     clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "\n",
    "#     # print(\"Naive Bayes\")\n",
    "#     # clf = CategoricalNB().fit(X_train, y_train)\n",
    "#     # scoring(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Baseline\")\n",
    "# eval_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# print(\"Random_OverSampling\")\n",
    "# X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"SMOTE\")\n",
    "# X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"ADASYN\")\n",
    "# X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"Random_UnderSampling\")\n",
    "# X_resampled, y_resampled = RandomUnderSampler().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Tree\n",
    "try:\n",
    "    params = {'splitter': ['best', 'random'],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "    best_tree, best_tree_params = find_best_hyperparam(DecisionTreeClassifier(), params, X_resampled, y_resampled)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "# try:\n",
    "#     params = {'alpha': np.logspace(0,-9, num=100)}\n",
    "#     best_nb, best_nb_params = find_best_hyperparam(CategoricalNB(min_categories=features.nunique()), params, X_resampled, y_resampled)\n",
    "#     print(best_nb.get_params())\n",
    "#     scoring(best_nb, X_val, y_val)\n",
    "#     print(best_nb_params)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ridge Regression\n",
    "\n",
    "# params = {'alpha': [0.1, 1.0, 10.0],\n",
    "#         'solver': ['auto', 'svd', 'cholesky','sparse_cg', 'saga', 'lbfgs']\n",
    "# }\n",
    "# best_ridgereg, best_ridgereg_params = find_best_hyperparam(RidgeClassifier(), params, X_resampled, y_resampled)\n",
    "# print(best_ridgereg.get_params())\n",
    "\n",
    "# try:\n",
    "#     scoring(best_ridgereg, X_val, y_val)\n",
    "#     print(best_ridgereg_params)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "\n",
    "\n",
    "best_forest_params = {'bootstrap': False, 'ccp_alpha': 0.0, \n",
    "                    'class_weight': None, 'criterion': 'gini', \n",
    "                    'max_depth': 70, \n",
    "                    'max_features': 'log2', \n",
    "                    'max_leaf_nodes': None, \n",
    "                    'max_samples': None, \n",
    "                    'min_impurity_decrease': 0.0, \n",
    "                    'min_samples_leaf': 1, \n",
    "                    'min_samples_split': 2, \n",
    "                    'min_weight_fraction_leaf': 0.0, \n",
    "                    'n_estimators': 600, \n",
    "                    'n_jobs': None, 'oob_score': False, \n",
    "                    'random_state': None, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "best_tree_params = {'ccp_alpha': 0.0, 'class_weight': None, \n",
    "                    'criterion': 'gini', 'max_depth': 30, \n",
    "                    'max_features': 'sqrt', 'max_leaf_nodes': None, \n",
    "                    'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, \n",
    "                    'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, \n",
    "                    'random_state': None, 'splitter': 'best'}\n",
    "\n",
    "best_logreg_params = {'C': 10, 'class_weight': None, 'dual': False, \n",
    "                      'fit_intercept': True, 'intercept_scaling': 1, \n",
    "                      'l1_ratio': None, 'max_iter': 100, \n",
    "                      'multi_class': 'auto', \n",
    "                      'n_jobs': None, 'penalty': 'l2', \n",
    "                      'random_state': None, 'solver': 'saga', \n",
    "                      'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "best_ridge_params = {'alpha': 0.1, 'class_weight': None, 'copy_X': True, \n",
    "                     'fit_intercept': True, 'max_iter': None, \n",
    "                     'positive': False, 'random_state': None, \n",
    "                     'solver': 'auto', 'tol': 0.0001}\n",
    "\n",
    "\n",
    "best_svc_params = {'C': 100, 'break_ties': False, 'cache_size': 200, \n",
    "                   'class_weight': None, 'coef0': 0.0, \n",
    "                   'decision_function_shape': 'ovr', 'degree': 3, \n",
    "                   'gamma': 0.001, 'kernel': 'rbf', \n",
    "                   'max_iter': 5000, 'probability': False, \n",
    "                   'random_state': None, 'shrinking': True, \n",
    "                   'tol': 0.001, 'verbose': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
