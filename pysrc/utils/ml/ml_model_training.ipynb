{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, auc, roc_auc_score, precision_score, balanced_accuracy_score, fbeta_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = pd.read_csv('features.csv')\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1, stratify=labels)\n",
    "\n",
    "y_train = y_train.to_numpy().ravel()\n",
    "y_test = y_test.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    print(f\"Recall:            {recall_score(y, y_pred)}\")\n",
    "    print(f\"Precision:         {precision_score(y, y_pred, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Note MDI is computed on training set\n",
    "# ## MDI has bias towards features with high cardinalities\n",
    "\n",
    "# def plot_MDI(forest):\n",
    "#     importances = forest.feature_importances_\n",
    "#     std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "#     forest_importances = pd.Series(importances, index=features.columns)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "#     ax.set_title(\"Feature importances using MDI\")\n",
    "#     ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "# def plot_permutation_feature_importance(forest, X_test, y_test):\n",
    "#     result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    \n",
    "#     forest_importances = pd.Series(result.importances_mean, index=features.columns)\n",
    "    \n",
    "#     fig, ax = plt.subplots()\n",
    "#     forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "#     ax.set_title(\"Feature importances using permutation on full model\")\n",
    "#     ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_models(X_train, y_train, X_test, y_test):\n",
    "#     print(\"Decision Tree\")\n",
    "#     clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "\n",
    "#     print(\"Random Forest\")\n",
    "#     clf = RandomForestClassifier(max_depth=4, random_state=0).fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "#     # plot_MDI(clf)\n",
    "#     # plot_permutation_feature_importance(clf, X_test, y_test)\n",
    "\n",
    "#     print(\"Logistic Regression\")\n",
    "#     clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "\n",
    "#     # print(\"Naive Bayes\")\n",
    "#     # clf = CategoricalNB().fit(X_train, y_train)\n",
    "#     # scoring(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Baseline\")\n",
    "# eval_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# print(\"Random_OverSampling\")\n",
    "# X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"SMOTE\")\n",
    "# X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"ADASYN\")\n",
    "# X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"Random_UnderSampling\")\n",
    "# X_resampled, y_resampled = RandomUnderSampler().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "fbeta_scorer = make_scorer(fbeta_score, beta=0.0001)\n",
    "\n",
    "def find_best_hyperparam(estimator, param_distributions, X_train, y_train):\n",
    "    clf = RandomizedSearchCV(estimator=estimator, \n",
    "                             param_distributions=param_distributions, \n",
    "                             scoring=fbeta_scorer,\n",
    "                             n_iter=20,\n",
    "                             cv=5, \n",
    "                             verbose=2, \n",
    "                             n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.best_estimator_, clf.best_params_\n",
    "\n",
    "X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "try:\n",
    "    params = {'splitter': ['best', 'random'],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "    best_tree, best_tree_params = find_best_hyperparam(DecisionTreeClassifier(), params, X_resampled, y_resampled)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logreg_cv = LogisticRegressionCV(cv=5, random_state=0).fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "try:\n",
    "    params = {'C': [1, 10, 100], \n",
    "          'gamma': [0.001, 0.0001], \n",
    "          'kernel': ['rbf', 'linear'],}\n",
    "\n",
    "    best_svc, best_svc_params = find_best_hyperparam(SVC(max_iter=5000, probability=True, class_weight = 'balanced'), params, X_resampled, y_resampled)\n",
    "    print(best_svc.get_params())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "# try:\n",
    "#     params = {'alpha': np.logspace(0,-9, num=100)}\n",
    "#     best_nb, best_nb_params = find_best_hyperparam(CategoricalNB(min_categories=features.nunique()), params, X_resampled, y_resampled)\n",
    "#     print(best_nb.get_params())\n",
    "#     scoring(best_nb, X_val, y_val)\n",
    "#     print(best_nb_params)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ridge Regression\n",
    "\n",
    "# params = {'alpha': [0.1, 1.0, 10.0],\n",
    "#         'solver': ['auto', 'svd', 'cholesky','sparse_cg', 'saga', 'lbfgs']\n",
    "# }\n",
    "# best_ridgereg, best_ridgereg_params = find_best_hyperparam(RidgeClassifier(), params, X_resampled, y_resampled)\n",
    "# print(best_ridgereg.get_params())\n",
    "\n",
    "# try:\n",
    "#     scoring(best_ridgereg, X_val, y_val)\n",
    "#     print(best_ridgereg_params)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "try:\n",
    "    params = {\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "    best_forest, best_tree_params = find_best_hyperparam(RandomForestClassifier(), params, X_resampled, y_resampled)\n",
    "    print(best_forest.get_params())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "print(\"Decision Tree\")\n",
    "scoring(best_tree, X_test, y_test)\n",
    "\n",
    "print(\"Forest\")\n",
    "scoring(best_forest, X_test, y_test)\n",
    "\n",
    "print(\"SVC\")\n",
    "scoring(best_svc, X_test, y_test)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "scoring(logreg_cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "\n",
    "\n",
    "best_forest_params = {'bootstrap': False, 'ccp_alpha': 0.0, \n",
    "                    'class_weight': None, 'criterion': 'gini', \n",
    "                    'max_depth': 70, \n",
    "                    'max_features': 'log2', \n",
    "                    'max_leaf_nodes': None, \n",
    "                    'max_samples': None, \n",
    "                    'min_impurity_decrease': 0.0, \n",
    "                    'min_samples_leaf': 1, \n",
    "                    'min_samples_split': 2, \n",
    "                    'min_weight_fraction_leaf': 0.0, \n",
    "                    'n_estimators': 600, \n",
    "                    'n_jobs': None, 'oob_score': False, \n",
    "                    'random_state': None, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "best_tree_params = {'ccp_alpha': 0.0, 'class_weight': None, \n",
    "                    'criterion': 'gini', 'max_depth': 30, \n",
    "                    'max_features': 'sqrt', 'max_leaf_nodes': None, \n",
    "                    'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, \n",
    "                    'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, \n",
    "                    'random_state': None, 'splitter': 'best'}\n",
    "\n",
    "best_logreg_params = {'C': 10, 'class_weight': None, 'dual': False, \n",
    "                      'fit_intercept': True, 'intercept_scaling': 1, \n",
    "                      'l1_ratio': None, 'max_iter': 100, \n",
    "                      'multi_class': 'auto', \n",
    "                      'n_jobs': None, 'penalty': 'l2', \n",
    "                      'random_state': None, 'solver': 'saga', \n",
    "                      'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "best_ridge_params = {'alpha': 0.1, 'class_weight': None, 'copy_X': True, \n",
    "                     'fit_intercept': True, 'max_iter': None, \n",
    "                     'positive': False, 'random_state': None, \n",
    "                     'solver': 'auto', 'tol': 0.0001}\n",
    "\n",
    "\n",
    "best_svc_params = {'C': 100, 'break_ties': False, 'cache_size': 200, \n",
    "                   'class_weight': None, 'coef0': 0.0, \n",
    "                   'decision_function_shape': 'ovr', 'degree': 3, \n",
    "                   'gamma': 0.001, 'kernel': 'rbf', \n",
    "                   'max_iter': 5000, 'probability': False, \n",
    "                   'random_state': None, 'shrinking': True, \n",
    "                   'tol': 0.001, 'verbose': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export models\n",
    "import pickle\n",
    "\n",
    "with open('decision_tree_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_tree, f)\n",
    "    \n",
    "# with open('forest_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_forest, f)\n",
    "    \n",
    "with open('svc_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_svc, f)\n",
    "\n",
    "with open('logreg_model.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_cv, f)\n",
    "    logreg_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
