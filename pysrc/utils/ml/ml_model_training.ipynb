{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score, auc, roc_auc_score, precision_score, balanced_accuracy_score, fbeta_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    print(f\"Recall:            {recall_score(y, y_pred)}\")\n",
    "    print(f\"Precision:         {precision_score(y, y_pred, zero_division=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_scorer = make_scorer(fbeta_score, beta=0.001)\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "def find_best_hyperparam(estimator, param_distributions, X_train, y_train):\n",
    "    clf = RandomizedSearchCV(estimator=estimator, \n",
    "                             param_distributions=param_distributions, \n",
    "                             scoring=f1_scorer,\n",
    "                             n_iter=15,\n",
    "                             cv=5, \n",
    "                             verbose=2, \n",
    "                             n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.best_estimator_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_SCOP\n",
      "Logistic Regression\n",
      "Forest\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=10.1min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=11.3min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=11.3min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=11.4min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=11.5min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1250; total time=10.4min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1250; total time=11.7min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1250; total time=11.8min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 4.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 4.6min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 4.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 4.5min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1250; total time=12.2min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=1250; total time=12.4min\n",
      "[CV] END max_depth=80, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=80, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 4.6min\n",
      "[CV] END max_depth=80, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.2min\n",
      "[CV] END max_depth=80, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 2.0min\n",
      "[CV] END max_depth=80, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 2.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 2.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time= 5.9min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time= 6.6min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time= 6.7min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time= 7.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=600; total time= 7.0min\n",
      "[CV] END max_depth=100, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=15.6min\n",
      "[CV] END max_depth=100, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=17.6min\n",
      "[CV] END max_depth=100, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=17.4min\n",
      "[CV] END max_depth=100, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=17.2min\n",
      "[CV] END max_depth=100, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1500; total time=15.9min\n",
      "[CV] END max_depth=100, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=2000; total time=18.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m40\u001b[39m, \u001b[39m60\u001b[39m, \u001b[39m80\u001b[39m, \u001b[39m100\u001b[39m],\n\u001b[1;32m     23\u001b[0m \u001b[39m'\u001b[39m\u001b[39mmax_features\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlog2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     24\u001b[0m \u001b[39m'\u001b[39m\u001b[39mmin_samples_leaf\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m],\n\u001b[1;32m     25\u001b[0m \u001b[39m'\u001b[39m\u001b[39mmin_samples_split\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m],\n\u001b[1;32m     26\u001b[0m \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m200\u001b[39m, \u001b[39m400\u001b[39m, \u001b[39m600\u001b[39m, \u001b[39m800\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m1250\u001b[39m, \u001b[39m1500\u001b[39m, \u001b[39m2000\u001b[39m]}\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mForest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m best_forest, best_tree_params \u001b[39m=\u001b[39m find_best_hyperparam(RandomForestClassifier(bootstrap\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), params, X_resampled, y_resampled)\n\u001b[1;32m     29\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m sub_folder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/forest_model_os.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m     pickle\u001b[39m.\u001b[39mdump(best_forest, f)\n",
      "Cell \u001b[0;32mIn [3], line 12\u001b[0m, in \u001b[0;36mfind_best_hyperparam\u001b[0;34m(estimator, param_distributions, X_train, y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_best_hyperparam\u001b[39m(estimator, param_distributions, X_train, y_train):\n\u001b[1;32m      5\u001b[0m     clf \u001b[39m=\u001b[39m RandomizedSearchCV(estimator\u001b[39m=\u001b[39mestimator, \n\u001b[1;32m      6\u001b[0m                              param_distributions\u001b[39m=\u001b[39mparam_distributions, \n\u001b[1;32m      7\u001b[0m                              scoring\u001b[39m=\u001b[39mf1_scorer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                              verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[1;32m     11\u001b[0m                              n_jobs\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m clf\u001b[39m.\u001b[39mbest_estimator_, clf\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1769\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1768\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1769\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1770\u001b[0m         ParameterSampler(\n\u001b[1;32m   1771\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1772\u001b[0m         )\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sub_folders = [f for f in os.listdir('.') if os.path.isdir(f) and 'no_' in f]\n",
    "\n",
    "for sub_folder in ['no_ENSEMBL', 'no_SCOP', 'no_TCPH']:\n",
    "    print(sub_folder)\n",
    "    \n",
    "    features = pd.read_csv('./' + sub_folder + '/features.csv', index_col=0)\n",
    "    labels = pd.read_csv('./' + sub_folder + '/labels.csv', index_col=0).to_numpy().ravel()\n",
    "    \n",
    "    # Scale\n",
    "    X_resampled, y_resampled = SMOTE().fit_resample(features, labels)\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    # Logistic Regression\n",
    "    print(\"Logistic Regression\")\n",
    "    from sklearn.linear_model import LogisticRegressionCV\n",
    "    logreg_cv = LogisticRegressionCV(cv=5, random_state=0, max_iter=1000, scoring=f1_scorer).fit(X_resampled, y_resampled)\n",
    "    with open('./' + sub_folder + '/logreg_model_os.pkl', 'wb') as f:\n",
    "        pickle.dump(logreg_cv, f)\n",
    "    \n",
    "    # # Random Forest\n",
    "    params = {\n",
    "    'max_depth': [10, 20, 30, 40, 60, 80, 100],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1250, 1500, 2000]}\n",
    "    print(\"Forest\")\n",
    "    best_forest, best_tree_params = find_best_hyperparam(RandomForestClassifier(bootstrap=True), params, X_resampled, y_resampled)\n",
    "    with open('./' + sub_folder + '/forest_model_os.pkl', 'wb') as f:\n",
    "        pickle.dump(best_forest, f)\n",
    "    \n",
    "    # Decision Tree\n",
    "    params = {'splitter': ['best', 'random'],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]}\n",
    "    print(\"DecisionTree\") \n",
    "    best_tree, best_tree_params = find_best_hyperparam(DecisionTreeClassifier(), params, X_resampled, y_resampled)\n",
    "    with open('./' + sub_folder + '/tree_model_os.pkl', 'wb') as f:\n",
    "        pickle.dump(best_tree, f)\n",
    "    \n",
    "    # SVC\n",
    "    # params = {'C': [1, 10, 100], \n",
    "    #       'gamma': [0.001, 0.0001], \n",
    "    #       'kernel': ['rbf', 'linear'],}\n",
    "    # print(\"SVC\")\n",
    "    # best_svc, best_svc_params = find_best_hyperparam(SVC(max_iter=5000, probability=True, class_weight = 'balanced'), params, X_resampled, y_resampled)\n",
    "    # with open('./' + sub_folder + '/svc_model.pkl', 'wb') as f:\n",
    "    #     pickle.dump(best_svc, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Note MDI is computed on training set\n",
    "# ## MDI has bias towards features with high cardinalities\n",
    "\n",
    "# def plot_MDI(forest):\n",
    "#     importances = forest.feature_importances_\n",
    "#     std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "#     forest_importances = pd.Series(importances, index=features.columns)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "#     ax.set_title(\"Feature importances using MDI\")\n",
    "#     ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "# def plot_permutation_feature_importance(forest, X_test, y_test):\n",
    "#     result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    \n",
    "#     forest_importances = pd.Series(result.importances_mean, index=features.columns)\n",
    "    \n",
    "#     fig, ax = plt.subplots()\n",
    "#     forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "#     ax.set_title(\"Feature importances using permutation on full model\")\n",
    "#     ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_models(X_train, y_train, X_test, y_test):\n",
    "#     print(\"Decision Tree\")\n",
    "#     clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "\n",
    "#     print(\"Random Forest\")\n",
    "#     clf = RandomForestClassifier(max_depth=4, random_state=0).fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "#     # plot_MDI(clf)\n",
    "#     # plot_permutation_feature_importance(clf, X_test, y_test)\n",
    "\n",
    "#     print(\"Logistic Regression\")\n",
    "#     clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#     scoring(clf, X_test, y_test)\n",
    "\n",
    "#     # print(\"Naive Bayes\")\n",
    "#     # clf = CategoricalNB().fit(X_train, y_train)\n",
    "#     # scoring(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Baseline\")\n",
    "# eval_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# print(\"Random_OverSampling\")\n",
    "# X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"SMOTE\")\n",
    "# X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"ADASYN\")\n",
    "# X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)\n",
    "\n",
    "# print(\"Random_UnderSampling\")\n",
    "# X_resampled, y_resampled = RandomUnderSampler().fit_resample(X_train, y_train)\n",
    "# eval_models(X_resampled, y_resampled, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Decision Tree\n",
    "# try:\n",
    "#     params = {'splitter': ['best', 'random'],\n",
    "#     'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "#     'max_features': ['log2', 'sqrt'],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "#     best_tree, best_tree_params = find_best_hyperparam(DecisionTreeClassifier(), params, X_resampled, y_resampled)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "# try:\n",
    "#     params = {'alpha': np.logspace(0,-9, num=100)}\n",
    "#     best_nb, best_nb_params = find_best_hyperparam(CategoricalNB(min_categories=features.nunique()), params, X_resampled, y_resampled)\n",
    "#     print(best_nb.get_params())\n",
    "#     scoring(best_nb, X_val, y_val)\n",
    "#     print(best_nb_params)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ridge Regression\n",
    "\n",
    "# params = {'alpha': [0.1, 1.0, 10.0],\n",
    "#         'solver': ['auto', 'svd', 'cholesky','sparse_cg', 'saga', 'lbfgs']\n",
    "# }\n",
    "# best_ridgereg, best_ridgereg_params = find_best_hyperparam(RidgeClassifier(), params, X_resampled, y_resampled)\n",
    "# print(best_ridgereg.get_params())\n",
    "\n",
    "# try:\n",
    "#     scoring(best_ridgereg, X_val, y_val)\n",
    "#     print(best_ridgereg_params)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Undersampling\n",
    "\n",
    "\n",
    "# best_forest_params = {'bootstrap': False, 'ccp_alpha': 0.0, \n",
    "#                     'class_weight': None, 'criterion': 'gini', \n",
    "#                     'max_depth': 70, \n",
    "#                     'max_features': 'log2', \n",
    "#                     'max_leaf_nodes': None, \n",
    "#                     'max_samples': None, \n",
    "#                     'min_impurity_decrease': 0.0, \n",
    "#                     'min_samples_leaf': 1, \n",
    "#                     'min_samples_split': 2, \n",
    "#                     'min_weight_fraction_leaf': 0.0, \n",
    "#                     'n_estimators': 600, \n",
    "#                     'n_jobs': None, 'oob_score': False, \n",
    "#                     'random_state': None, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "# best_tree_params = {'ccp_alpha': 0.0, 'class_weight': None, \n",
    "#                     'criterion': 'gini', 'max_depth': 30, \n",
    "#                     'max_features': 'sqrt', 'max_leaf_nodes': None, \n",
    "#                     'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, \n",
    "#                     'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, \n",
    "#                     'random_state': None, 'splitter': 'best'}\n",
    "\n",
    "# best_logreg_params = {'C': 10, 'class_weight': None, 'dual': False, \n",
    "#                       'fit_intercept': True, 'intercept_scaling': 1, \n",
    "#                       'l1_ratio': None, 'max_iter': 100, \n",
    "#                       'multi_class': 'auto', \n",
    "#                       'n_jobs': None, 'penalty': 'l2', \n",
    "#                       'random_state': None, 'solver': 'saga', \n",
    "#                       'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "# best_ridge_params = {'alpha': 0.1, 'class_weight': None, 'copy_X': True, \n",
    "#                      'fit_intercept': True, 'max_iter': None, \n",
    "#                      'positive': False, 'random_state': None, \n",
    "#                      'solver': 'auto', 'tol': 0.0001}\n",
    "\n",
    "\n",
    "# best_svc_params = {'C': 100, 'break_ties': False, 'cache_size': 200, \n",
    "#                    'class_weight': None, 'coef0': 0.0, \n",
    "#                    'decision_function_shape': 'ovr', 'degree': 3, \n",
    "#                    'gamma': 0.001, 'kernel': 'rbf', \n",
    "#                    'max_iter': 5000, 'probability': False, \n",
    "#                    'random_state': None, 'shrinking': True, \n",
    "#                    'tol': 0.001, 'verbose': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
